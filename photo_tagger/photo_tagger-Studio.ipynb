{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Photo Tagger with PyTorch in SageMaker Studio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6DOfs2ckx0lU",
    "outputId": "c3fab771-4f7c-4f70-8a5b-f73db6711108"
   },
   "source": [
    "Let's use the sample dataset \"house-rooms-image-dataset\" from Kaggle:\n",
    "https://www.kaggle.com/datasets/robinreni/house-rooms-image-dataset\n",
    "\n",
    "we will download it using the Kaggle API... (make sure you download your kaggle.json file first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install kaggle --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
      "Downloading house-rooms-image-dataset.zip to /root/house-rooms/photo_tagger\n",
      "100%|███████████████████████████████████████▊| 116M/116M [00:04<00:00, 37.4MB/s]\n",
      "100%|████████████████████████████████████████| 116M/116M [00:05<00:00, 22.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "!mkdir ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d robinreni/house-rooms-image-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!unzip -q house-rooms-image-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install split-folders --quiet\n",
    "!pip install livelossplot --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 5250 files [01:55, 45.61 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "splitfolders.ratio('House_Room_Dataset', output=\"Split_Dataset\", seed=1337, ratio=(.8, 0.1,0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xv_fYlxuvPt4",
    "outputId": "c93b9039-487b-47c8-ea7f-af6791ef76a7",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "#*********************\n",
    "#!cp drive/MyDrive/10_PROJECTS/photo_tagger/helpers.py .\n",
    "#!cp drive/MyDrive/10_PROJECTS/photo_tagger/mean_and_std.pt .\n",
    "\n",
    "import torchvision.datasets\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models\n",
    "import torch.optim\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "#from helpers import *\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "a0h7a3ljwKHh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SETTINGS\n",
    "#**************************************************\n",
    "batch_size = 32        # size of the minibatch for stochastic gradient descent (or Adam)\n",
    "valid_size = 0.2       # fraction of the training data to reserve for validation\n",
    "n_epochs = 5        # number of epochs for training\n",
    "#dropout = 0.4          # dropout for our model\n",
    "learning_rate = 0.05  # Learning rate for SGD (or Adam)\n",
    "momentum = 0.5 \n",
    "weight_decay = 0.01     # regularization. Increase this to combat overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iMeNsQPwjtS",
    "outputId": "28645578-3cbc-4fa8-8512-bfa02d5148a3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4198 522 530 5\n"
     ]
    }
   ],
   "source": [
    "# DATA LOADERS\n",
    "#*********************\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# Load the Data\n",
    "# Set train and valid directory paths\n",
    "train_directory = 'Split_Dataset/train'\n",
    "valid_directory = 'Split_Dataset/val'\n",
    "test_directory = 'Split_Dataset/test'\n",
    "# Batch size\n",
    "bs = 32\n",
    "# Number of classes\n",
    "num_classes = 5\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # you can add other transformations in this list\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load Data from folders\n",
    "data = {\n",
    "    'train': datasets.ImageFolder(root=train_directory, transform=transform),\n",
    "    'valid': datasets.ImageFolder(root=valid_directory, transform=transform),\n",
    "    'test': datasets.ImageFolder(root=test_directory, transform=transform)\n",
    "    #'train': datasets.ImageFolder(root=train_directory),\n",
    "    #'valid': datasets.ImageFolder(root=valid_directory),\n",
    "    #'test': datasets.ImageFolder(root=test_directory)\n",
    "}\n",
    "# Size of Data, to be used for calculating Average Loss and Accuracy\n",
    "train_data_size = len(data['train'])\n",
    "valid_data_size = len(data['valid'])\n",
    "test_data_size = len(data['test'])\n",
    "# Create iterators for the Data loaded using DataLoader module\n",
    "train = DataLoader(data['train'], batch_size=bs, shuffle=True)\n",
    "valid = DataLoader(data['valid'], batch_size=bs, shuffle=True)\n",
    "test = DataLoader(data['test'], batch_size=bs, shuffle=True)\n",
    "\n",
    "#data_loaders = get_data_loaders(batch_size=batch_size,valid_size=valid_size)\n",
    "data_loaders={\n",
    "    \"train\": train,\n",
    "    \"valid\": valid,\n",
    "    \"test\": test\n",
    "}\n",
    "n_classes = len(data_loaders[\"train\"].dataset.classes)\n",
    "\n",
    "# Print the train, validation and test set data sizes\n",
    "print(train_data_size, valid_data_size, test_data_size, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173,
     "referenced_widgets": [
      "8935e48ec10441be950c56e86b2ddda5",
      "21d94565226c4382a1246fc35ee07af3",
      "1a9fbc0036d54628adabdb11333ac239",
      "832c967da8344db58ef0c6ee528ec936",
      "bc723b18a6eb4332a77b371e77244b34",
      "6b38c4778ce84a898c3d8a48c551868e",
      "48fbf4f41d504a1c80c2ebef4f7cf89c",
      "5fd5a2129451462fbaf6102ebcbdf5e4",
      "de62d745176a45cbb717a4ac0e01c663",
      "65c94d6ae1ad4259b4a2bf6eae9a05a3",
      "bf5dee7f55aa41089f82f50415a2d34d"
     ]
    },
    "id": "RA7PjPdey0y1",
    "outputId": "f3790b1a-74cb-479e-9308-a227ec8083bc",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze 161 groups of parameters\n"
     ]
    }
   ],
   "source": [
    "# TRANSFER MODEL: FREEZE BACKBONE AND THAW THE HEAD\n",
    "#**************************************************\n",
    "\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "n_inputs = model.fc.in_features\n",
    "\n",
    "# Feel free to experiment with more complicated heads\n",
    "model.fc = nn.Linear(n_inputs, n_classes)\n",
    "\n",
    "frozen_parameters = []\n",
    "for p in model.parameters():\n",
    "    # Freeze only parameters that are not already frozen\n",
    "    # (if any)\n",
    "    if p.requires_grad:\n",
    "        p.requires_grad = False\n",
    "        frozen_parameters.append(p)\n",
    "\n",
    "print(f\"Froze {len(frozen_parameters)} groups of parameters\")\n",
    "\n",
    "# Now let's thaw the parameters of the head we have\n",
    "# added\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VSVgwhiny5vr",
    "outputId": "fb393dd4-5d7f-477d-cc61-5cdd9525dbb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAIN, VALIDATION\n",
    "#*********************************\n",
    "loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight_decay)\n",
    "\n",
    "#model.load_state_dict(torch.load('/root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7X-_7lcVMVbj",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#optimize(\n",
    "#    data_loaders,\n",
    "#    model,\n",
    "#    optimizer,\n",
    "#    loss,\n",
    "#    n_epochs=n_epochs,\n",
    "#    save_path=\"./checkpoints/model_estimated.pt\",\n",
    "#    interactive_tracking=False\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "[2023-02-27 11:28:51.218 1-8-1-cpu-py36-ml-g4dn-xlarge-5b0c5292be3b9805b73bbd06ca03:34 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\n",
      "[2023-02-27 11:28:51.391 1-8-1-cpu-py36-ml-g4dn-xlarge-5b0c5292be3b9805b73bbd06ca03:34 INFO profiler_config_parser.py:102] Unable to find config at /opt/ml/input/config/profilerconfig.json. Profiler is disabled.\n",
      "Batch number: 000, Training: Loss: 1.6514, Accuracy: 0.1562\n",
      "Batch number: 001, Training: Loss: 3.2754, Accuracy: 0.2188\n",
      "Batch number: 002, Training: Loss: 4.1699, Accuracy: 0.2812\n",
      "Batch number: 003, Training: Loss: 6.9756, Accuracy: 0.2188\n",
      "Batch number: 004, Training: Loss: 7.1840, Accuracy: 0.1250\n",
      "Batch number: 005, Training: Loss: 10.1308, Accuracy: 0.2188\n",
      "Batch number: 006, Training: Loss: 6.8404, Accuracy: 0.2188\n",
      "Batch number: 007, Training: Loss: 8.1000, Accuracy: 0.1562\n",
      "Batch number: 008, Training: Loss: 7.4743, Accuracy: 0.2500\n",
      "Batch number: 009, Training: Loss: 12.2053, Accuracy: 0.2812\n",
      "Batch number: 010, Training: Loss: 7.0701, Accuracy: 0.1250\n",
      "Batch number: 011, Training: Loss: 3.3711, Accuracy: 0.3125\n",
      "Batch number: 012, Training: Loss: 7.1038, Accuracy: 0.2500\n",
      "Batch number: 013, Training: Loss: 5.4899, Accuracy: 0.1562\n",
      "Batch number: 014, Training: Loss: 9.2012, Accuracy: 0.1875\n",
      "Batch number: 015, Training: Loss: 5.2697, Accuracy: 0.3125\n",
      "Batch number: 016, Training: Loss: 4.3810, Accuracy: 0.4688\n",
      "Batch number: 017, Training: Loss: 6.1644, Accuracy: 0.2812\n",
      "Batch number: 018, Training: Loss: 3.6955, Accuracy: 0.4375\n",
      "Batch number: 019, Training: Loss: 4.4620, Accuracy: 0.1875\n",
      "Batch number: 020, Training: Loss: 1.4150, Accuracy: 0.5312\n",
      "Batch number: 021, Training: Loss: 4.0989, Accuracy: 0.2500\n",
      "Batch number: 022, Training: Loss: 3.6603, Accuracy: 0.4062\n",
      "Batch number: 023, Training: Loss: 5.2457, Accuracy: 0.3750\n",
      "Batch number: 024, Training: Loss: 2.7239, Accuracy: 0.4375\n",
      "Batch number: 025, Training: Loss: 2.4060, Accuracy: 0.4375\n",
      "Batch number: 026, Training: Loss: 3.3453, Accuracy: 0.3125\n",
      "Batch number: 027, Training: Loss: 4.0530, Accuracy: 0.1875\n",
      "Batch number: 028, Training: Loss: 5.7275, Accuracy: 0.3125\n",
      "Batch number: 029, Training: Loss: 5.5225, Accuracy: 0.2812\n",
      "Batch number: 030, Training: Loss: 3.5647, Accuracy: 0.2812\n",
      "Batch number: 031, Training: Loss: 4.1540, Accuracy: 0.2812\n",
      "Batch number: 032, Training: Loss: 2.7992, Accuracy: 0.3438\n",
      "Batch number: 033, Training: Loss: 3.8921, Accuracy: 0.4062\n",
      "Batch number: 034, Training: Loss: 2.9754, Accuracy: 0.3750\n",
      "Batch number: 035, Training: Loss: 3.3271, Accuracy: 0.3125\n",
      "Batch number: 036, Training: Loss: 2.8208, Accuracy: 0.5312\n",
      "Batch number: 037, Training: Loss: 1.5495, Accuracy: 0.6562\n",
      "Batch number: 038, Training: Loss: 1.2499, Accuracy: 0.6562\n",
      "Batch number: 039, Training: Loss: 0.9169, Accuracy: 0.6562\n",
      "Batch number: 040, Training: Loss: 1.8509, Accuracy: 0.5000\n",
      "Batch number: 041, Training: Loss: 1.8685, Accuracy: 0.5625\n",
      "Batch number: 042, Training: Loss: 1.8132, Accuracy: 0.6562\n",
      "Batch number: 043, Training: Loss: 1.2039, Accuracy: 0.5938\n",
      "Batch number: 044, Training: Loss: 1.6103, Accuracy: 0.5625\n",
      "Batch number: 045, Training: Loss: 1.0699, Accuracy: 0.6562\n",
      "Batch number: 046, Training: Loss: 0.9766, Accuracy: 0.7812\n",
      "Batch number: 047, Training: Loss: 1.1440, Accuracy: 0.6875\n",
      "Batch number: 048, Training: Loss: 1.6565, Accuracy: 0.4688\n",
      "Batch number: 049, Training: Loss: 2.7152, Accuracy: 0.4688\n",
      "Batch number: 050, Training: Loss: 1.8335, Accuracy: 0.5312\n",
      "Batch number: 051, Training: Loss: 3.9657, Accuracy: 0.3438\n",
      "Batch number: 052, Training: Loss: 0.9869, Accuracy: 0.6875\n",
      "Batch number: 053, Training: Loss: 1.4722, Accuracy: 0.5312\n",
      "Batch number: 054, Training: Loss: 0.6335, Accuracy: 0.7500\n",
      "Batch number: 055, Training: Loss: 0.8516, Accuracy: 0.7188\n",
      "Batch number: 056, Training: Loss: 1.9034, Accuracy: 0.3750\n",
      "Batch number: 057, Training: Loss: 1.7661, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 1.9385, Accuracy: 0.4688\n",
      "Batch number: 059, Training: Loss: 2.0744, Accuracy: 0.4688\n",
      "Batch number: 060, Training: Loss: 1.3287, Accuracy: 0.5625\n",
      "Batch number: 061, Training: Loss: 1.5490, Accuracy: 0.5000\n",
      "Batch number: 062, Training: Loss: 1.2564, Accuracy: 0.6562\n",
      "Batch number: 063, Training: Loss: 1.7935, Accuracy: 0.5312\n",
      "Batch number: 064, Training: Loss: 1.2844, Accuracy: 0.5938\n",
      "Batch number: 065, Training: Loss: 2.1839, Accuracy: 0.5938\n",
      "Batch number: 066, Training: Loss: 2.1438, Accuracy: 0.4688\n",
      "Batch number: 067, Training: Loss: 2.1541, Accuracy: 0.4062\n",
      "Batch number: 068, Training: Loss: 2.1507, Accuracy: 0.4375\n",
      "Batch number: 069, Training: Loss: 1.7622, Accuracy: 0.4688\n",
      "Batch number: 070, Training: Loss: 1.0887, Accuracy: 0.5625\n",
      "Batch number: 071, Training: Loss: 0.5980, Accuracy: 0.7812\n",
      "Batch number: 072, Training: Loss: 1.0507, Accuracy: 0.6562\n",
      "Batch number: 073, Training: Loss: 1.0022, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 1.3022, Accuracy: 0.5312\n",
      "Batch number: 075, Training: Loss: 0.7418, Accuracy: 0.8125\n",
      "Batch number: 076, Training: Loss: 1.8904, Accuracy: 0.6875\n",
      "Batch number: 077, Training: Loss: 1.3731, Accuracy: 0.5312\n",
      "Batch number: 078, Training: Loss: 1.4115, Accuracy: 0.6250\n",
      "Batch number: 079, Training: Loss: 1.2474, Accuracy: 0.6250\n",
      "Batch number: 080, Training: Loss: 1.2176, Accuracy: 0.7188\n",
      "Batch number: 081, Training: Loss: 0.7355, Accuracy: 0.7812\n",
      "Batch number: 082, Training: Loss: 1.1753, Accuracy: 0.5625\n",
      "Batch number: 083, Training: Loss: 1.4060, Accuracy: 0.5938\n",
      "Batch number: 084, Training: Loss: 1.8246, Accuracy: 0.5000\n",
      "Batch number: 085, Training: Loss: 1.9049, Accuracy: 0.4375\n",
      "Batch number: 086, Training: Loss: 2.6653, Accuracy: 0.5000\n",
      "Batch number: 087, Training: Loss: 0.7683, Accuracy: 0.7188\n",
      "Batch number: 088, Training: Loss: 1.2871, Accuracy: 0.5625\n",
      "Batch number: 089, Training: Loss: 2.0111, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 0.6759, Accuracy: 0.7500\n",
      "Batch number: 091, Training: Loss: 1.4761, Accuracy: 0.5938\n",
      "Batch number: 092, Training: Loss: 3.0119, Accuracy: 0.4062\n",
      "Batch number: 093, Training: Loss: 1.3767, Accuracy: 0.6875\n",
      "Batch number: 094, Training: Loss: 3.3002, Accuracy: 0.5000\n",
      "Batch number: 095, Training: Loss: 2.1959, Accuracy: 0.4688\n",
      "Batch number: 096, Training: Loss: 1.4776, Accuracy: 0.5000\n",
      "Batch number: 097, Training: Loss: 1.3244, Accuracy: 0.5312\n",
      "Batch number: 098, Training: Loss: 0.9508, Accuracy: 0.7188\n",
      "Batch number: 099, Training: Loss: 0.6385, Accuracy: 0.8125\n",
      "Batch number: 100, Training: Loss: 1.0189, Accuracy: 0.6875\n",
      "Batch number: 101, Training: Loss: 0.7036, Accuracy: 0.7812\n",
      "Batch number: 102, Training: Loss: 0.7434, Accuracy: 0.6875\n",
      "Batch number: 103, Training: Loss: 2.0876, Accuracy: 0.5312\n",
      "Batch number: 104, Training: Loss: 1.3972, Accuracy: 0.5938\n",
      "Batch number: 105, Training: Loss: 1.6013, Accuracy: 0.6562\n",
      "Batch number: 106, Training: Loss: 1.4795, Accuracy: 0.5312\n",
      "Batch number: 107, Training: Loss: 0.8275, Accuracy: 0.6875\n",
      "Batch number: 108, Training: Loss: 1.2025, Accuracy: 0.6562\n",
      "Batch number: 109, Training: Loss: 0.5933, Accuracy: 0.7500\n",
      "Batch number: 110, Training: Loss: 0.8091, Accuracy: 0.6875\n",
      "Batch number: 111, Training: Loss: 0.3711, Accuracy: 0.8438\n",
      "Batch number: 112, Training: Loss: 0.4529, Accuracy: 0.9062\n",
      "Batch number: 113, Training: Loss: 1.1610, Accuracy: 0.6250\n",
      "Batch number: 114, Training: Loss: 0.7167, Accuracy: 0.7188\n",
      "Batch number: 115, Training: Loss: 1.3252, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 1.4485, Accuracy: 0.5312\n",
      "Batch number: 117, Training: Loss: 1.1631, Accuracy: 0.7188\n",
      "Batch number: 118, Training: Loss: 1.0641, Accuracy: 0.6250\n",
      "Batch number: 119, Training: Loss: 0.8577, Accuracy: 0.7188\n",
      "Batch number: 120, Training: Loss: 0.5162, Accuracy: 0.9062\n",
      "Batch number: 121, Training: Loss: 0.7031, Accuracy: 0.7812\n",
      "Batch number: 122, Training: Loss: 0.4173, Accuracy: 0.8750\n",
      "Batch number: 123, Training: Loss: 0.7748, Accuracy: 0.8125\n",
      "Batch number: 124, Training: Loss: 0.5192, Accuracy: 0.7812\n",
      "Batch number: 125, Training: Loss: 1.1233, Accuracy: 0.6875\n",
      "Batch number: 126, Training: Loss: 0.9389, Accuracy: 0.7188\n",
      "Batch number: 127, Training: Loss: 1.2574, Accuracy: 0.5938\n",
      "Batch number: 128, Training: Loss: 2.3255, Accuracy: 0.5000\n",
      "Batch number: 129, Training: Loss: 1.1701, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 2.0410, Accuracy: 0.5312\n",
      "Batch number: 131, Training: Loss: 1.9774, Accuracy: 0.5000\n",
      "Epoch: 2/5\n",
      "Batch number: 000, Training: Loss: 4.4404, Accuracy: 0.4062\n",
      "Batch number: 001, Training: Loss: 4.1838, Accuracy: 0.5312\n",
      "Batch number: 002, Training: Loss: 1.9564, Accuracy: 0.4062\n",
      "Batch number: 003, Training: Loss: 1.1677, Accuracy: 0.6250\n",
      "Batch number: 004, Training: Loss: 0.8399, Accuracy: 0.6875\n",
      "Batch number: 005, Training: Loss: 0.9685, Accuracy: 0.7812\n",
      "Batch number: 006, Training: Loss: 0.4831, Accuracy: 0.8125\n",
      "Batch number: 007, Training: Loss: 1.4755, Accuracy: 0.5312\n",
      "Batch number: 008, Training: Loss: 3.5578, Accuracy: 0.3125\n",
      "Batch number: 009, Training: Loss: 1.1887, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 4.7487, Accuracy: 0.6250\n",
      "Batch number: 011, Training: Loss: 2.1590, Accuracy: 0.5938\n",
      "Batch number: 012, Training: Loss: 1.3581, Accuracy: 0.5625\n",
      "Batch number: 013, Training: Loss: 3.4302, Accuracy: 0.1875\n",
      "Batch number: 014, Training: Loss: 2.4766, Accuracy: 0.6250\n",
      "Batch number: 015, Training: Loss: 2.3763, Accuracy: 0.5625\n",
      "Batch number: 016, Training: Loss: 1.6941, Accuracy: 0.5625\n",
      "Batch number: 017, Training: Loss: 1.0893, Accuracy: 0.6250\n",
      "Batch number: 018, Training: Loss: 2.5104, Accuracy: 0.5000\n",
      "Batch number: 019, Training: Loss: 1.0110, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.5103, Accuracy: 0.8125\n",
      "Batch number: 021, Training: Loss: 0.4380, Accuracy: 0.8125\n",
      "Batch number: 022, Training: Loss: 1.3701, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 1.0659, Accuracy: 0.7188\n",
      "Batch number: 024, Training: Loss: 2.0534, Accuracy: 0.6562\n",
      "Batch number: 025, Training: Loss: 1.1311, Accuracy: 0.6250\n",
      "Batch number: 026, Training: Loss: 0.7608, Accuracy: 0.7188\n",
      "Batch number: 027, Training: Loss: 1.2846, Accuracy: 0.6250\n",
      "Batch number: 028, Training: Loss: 0.9136, Accuracy: 0.7812\n",
      "Batch number: 029, Training: Loss: 0.6879, Accuracy: 0.7500\n",
      "Batch number: 030, Training: Loss: 0.6116, Accuracy: 0.7188\n",
      "Batch number: 031, Training: Loss: 0.9942, Accuracy: 0.6875\n",
      "Batch number: 032, Training: Loss: 0.8981, Accuracy: 0.7188\n",
      "Batch number: 033, Training: Loss: 1.4719, Accuracy: 0.5938\n",
      "Batch number: 034, Training: Loss: 0.7890, Accuracy: 0.7812\n",
      "Batch number: 035, Training: Loss: 1.4879, Accuracy: 0.6250\n",
      "Batch number: 036, Training: Loss: 0.6135, Accuracy: 0.8125\n",
      "Batch number: 037, Training: Loss: 1.0716, Accuracy: 0.5938\n",
      "Batch number: 038, Training: Loss: 0.6526, Accuracy: 0.8438\n",
      "Batch number: 039, Training: Loss: 1.8003, Accuracy: 0.6250\n",
      "Batch number: 040, Training: Loss: 0.9914, Accuracy: 0.7188\n",
      "Batch number: 041, Training: Loss: 1.8211, Accuracy: 0.4375\n",
      "Batch number: 042, Training: Loss: 1.1599, Accuracy: 0.7188\n",
      "Batch number: 043, Training: Loss: 2.1278, Accuracy: 0.4688\n",
      "Batch number: 044, Training: Loss: 1.9752, Accuracy: 0.5938\n",
      "Batch number: 045, Training: Loss: 0.6969, Accuracy: 0.7188\n",
      "Batch number: 046, Training: Loss: 1.4369, Accuracy: 0.6250\n",
      "Batch number: 047, Training: Loss: 0.2358, Accuracy: 0.9375\n",
      "Batch number: 048, Training: Loss: 0.4880, Accuracy: 0.8750\n",
      "Batch number: 049, Training: Loss: 1.1042, Accuracy: 0.7188\n",
      "Batch number: 050, Training: Loss: 0.7906, Accuracy: 0.6875\n",
      "Batch number: 051, Training: Loss: 1.0312, Accuracy: 0.6875\n",
      "Batch number: 052, Training: Loss: 1.4028, Accuracy: 0.7500\n",
      "Batch number: 053, Training: Loss: 0.6905, Accuracy: 0.8750\n",
      "Batch number: 054, Training: Loss: 1.2503, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 0.6345, Accuracy: 0.7500\n",
      "Batch number: 056, Training: Loss: 0.6227, Accuracy: 0.8125\n",
      "Batch number: 057, Training: Loss: 0.6326, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 1.0290, Accuracy: 0.6875\n",
      "Batch number: 059, Training: Loss: 1.0179, Accuracy: 0.6562\n",
      "Batch number: 060, Training: Loss: 1.7916, Accuracy: 0.5938\n",
      "Batch number: 061, Training: Loss: 0.8743, Accuracy: 0.7812\n",
      "Batch number: 062, Training: Loss: 1.1729, Accuracy: 0.5938\n",
      "Batch number: 063, Training: Loss: 0.6150, Accuracy: 0.7812\n",
      "Batch number: 064, Training: Loss: 0.6360, Accuracy: 0.8438\n",
      "Batch number: 065, Training: Loss: 0.8207, Accuracy: 0.6250\n",
      "Batch number: 066, Training: Loss: 1.6887, Accuracy: 0.5312\n",
      "Batch number: 067, Training: Loss: 1.7573, Accuracy: 0.6875\n",
      "Batch number: 068, Training: Loss: 1.8103, Accuracy: 0.5312\n",
      "Batch number: 069, Training: Loss: 1.8883, Accuracy: 0.4688\n",
      "Batch number: 070, Training: Loss: 1.2692, Accuracy: 0.6875\n",
      "Batch number: 071, Training: Loss: 1.9333, Accuracy: 0.6250\n",
      "Batch number: 072, Training: Loss: 0.9951, Accuracy: 0.6875\n",
      "Batch number: 073, Training: Loss: 0.6011, Accuracy: 0.7812\n",
      "Batch number: 074, Training: Loss: 1.8092, Accuracy: 0.5938\n",
      "Batch number: 075, Training: Loss: 1.0244, Accuracy: 0.7188\n",
      "Batch number: 076, Training: Loss: 0.3998, Accuracy: 0.8438\n",
      "Batch number: 077, Training: Loss: 0.4233, Accuracy: 0.7812\n",
      "Batch number: 078, Training: Loss: 0.6206, Accuracy: 0.7500\n",
      "Batch number: 079, Training: Loss: 1.8616, Accuracy: 0.4688\n",
      "Batch number: 080, Training: Loss: 2.5034, Accuracy: 0.5938\n",
      "Batch number: 081, Training: Loss: 0.7909, Accuracy: 0.7188\n",
      "Batch number: 082, Training: Loss: 1.6778, Accuracy: 0.6250\n",
      "Batch number: 083, Training: Loss: 1.7597, Accuracy: 0.5625\n",
      "Batch number: 084, Training: Loss: 0.9982, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.6704, Accuracy: 0.7812\n",
      "Batch number: 086, Training: Loss: 1.3011, Accuracy: 0.5000\n",
      "Batch number: 087, Training: Loss: 1.3994, Accuracy: 0.5938\n",
      "Batch number: 088, Training: Loss: 2.1439, Accuracy: 0.5625\n",
      "Batch number: 089, Training: Loss: 0.5349, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.8645, Accuracy: 0.6875\n",
      "Batch number: 091, Training: Loss: 0.6997, Accuracy: 0.7500\n",
      "Batch number: 092, Training: Loss: 0.7529, Accuracy: 0.6875\n",
      "Batch number: 093, Training: Loss: 0.9567, Accuracy: 0.7188\n",
      "Batch number: 094, Training: Loss: 0.2162, Accuracy: 0.9375\n",
      "Batch number: 095, Training: Loss: 1.3088, Accuracy: 0.5625\n",
      "Batch number: 096, Training: Loss: 2.0458, Accuracy: 0.5312\n",
      "Batch number: 097, Training: Loss: 1.9212, Accuracy: 0.6250\n",
      "Batch number: 098, Training: Loss: 0.4751, Accuracy: 0.8125\n",
      "Batch number: 099, Training: Loss: 2.7871, Accuracy: 0.4688\n",
      "Batch number: 100, Training: Loss: 1.4755, Accuracy: 0.5625\n",
      "Batch number: 101, Training: Loss: 1.5377, Accuracy: 0.5312\n",
      "Batch number: 102, Training: Loss: 0.8353, Accuracy: 0.8125\n",
      "Batch number: 103, Training: Loss: 1.9977, Accuracy: 0.4375\n",
      "Batch number: 104, Training: Loss: 1.4365, Accuracy: 0.6250\n",
      "Batch number: 105, Training: Loss: 0.9949, Accuracy: 0.6875\n",
      "Batch number: 106, Training: Loss: 0.3450, Accuracy: 0.8438\n",
      "Batch number: 107, Training: Loss: 0.6932, Accuracy: 0.8125\n",
      "Batch number: 108, Training: Loss: 1.1387, Accuracy: 0.6250\n",
      "Batch number: 109, Training: Loss: 0.6241, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 0.5342, Accuracy: 0.8438\n",
      "Batch number: 111, Training: Loss: 0.8017, Accuracy: 0.7500\n",
      "Batch number: 112, Training: Loss: 0.5360, Accuracy: 0.7812\n",
      "Batch number: 113, Training: Loss: 0.6047, Accuracy: 0.7500\n",
      "Batch number: 114, Training: Loss: 0.8096, Accuracy: 0.6875\n",
      "Batch number: 115, Training: Loss: 1.0845, Accuracy: 0.6250\n",
      "Batch number: 116, Training: Loss: 0.5296, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 0.7750, Accuracy: 0.7500\n",
      "Batch number: 118, Training: Loss: 0.9337, Accuracy: 0.6562\n",
      "Batch number: 119, Training: Loss: 0.9158, Accuracy: 0.6250\n",
      "Batch number: 120, Training: Loss: 1.3118, Accuracy: 0.5312\n",
      "Batch number: 121, Training: Loss: 0.8599, Accuracy: 0.7500\n",
      "Batch number: 122, Training: Loss: 1.2868, Accuracy: 0.5625\n",
      "Batch number: 123, Training: Loss: 0.9659, Accuracy: 0.7500\n",
      "Batch number: 124, Training: Loss: 2.1036, Accuracy: 0.6562\n",
      "Batch number: 125, Training: Loss: 0.5558, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 1.0414, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 1.0603, Accuracy: 0.7188\n",
      "Batch number: 128, Training: Loss: 0.6602, Accuracy: 0.7188\n",
      "Batch number: 129, Training: Loss: 0.6930, Accuracy: 0.7500\n",
      "Batch number: 130, Training: Loss: 1.2878, Accuracy: 0.6250\n",
      "Batch number: 131, Training: Loss: 2.8283, Accuracy: 0.5000\n",
      "Epoch: 3/5\n",
      "Batch number: 000, Training: Loss: 1.6575, Accuracy: 0.5938\n",
      "Batch number: 001, Training: Loss: 0.4891, Accuracy: 0.8750\n",
      "Batch number: 002, Training: Loss: 0.8642, Accuracy: 0.7500\n",
      "Batch number: 003, Training: Loss: 0.9851, Accuracy: 0.7188\n",
      "Batch number: 004, Training: Loss: 0.7065, Accuracy: 0.7812\n",
      "Batch number: 005, Training: Loss: 0.6815, Accuracy: 0.6875\n",
      "Batch number: 006, Training: Loss: 1.4150, Accuracy: 0.5625\n",
      "Batch number: 007, Training: Loss: 0.6309, Accuracy: 0.7812\n",
      "Batch number: 008, Training: Loss: 0.2865, Accuracy: 0.8750\n",
      "Batch number: 009, Training: Loss: 1.0446, Accuracy: 0.6875\n",
      "Batch number: 010, Training: Loss: 1.1430, Accuracy: 0.5938\n",
      "Batch number: 011, Training: Loss: 1.2060, Accuracy: 0.7812\n",
      "Batch number: 012, Training: Loss: 1.9924, Accuracy: 0.5938\n",
      "Batch number: 013, Training: Loss: 0.7411, Accuracy: 0.7812\n",
      "Batch number: 014, Training: Loss: 1.3296, Accuracy: 0.5312\n",
      "Batch number: 015, Training: Loss: 0.8688, Accuracy: 0.7188\n",
      "Batch number: 016, Training: Loss: 0.9879, Accuracy: 0.6562\n",
      "Batch number: 017, Training: Loss: 0.3607, Accuracy: 0.8438\n",
      "Batch number: 018, Training: Loss: 0.9561, Accuracy: 0.6875\n",
      "Batch number: 019, Training: Loss: 0.5408, Accuracy: 0.8125\n",
      "Batch number: 020, Training: Loss: 1.3905, Accuracy: 0.6562\n",
      "Batch number: 021, Training: Loss: 1.8243, Accuracy: 0.5625\n",
      "Batch number: 022, Training: Loss: 3.3922, Accuracy: 0.3750\n",
      "Batch number: 023, Training: Loss: 2.1591, Accuracy: 0.4375\n",
      "Batch number: 024, Training: Loss: 0.9200, Accuracy: 0.6875\n",
      "Batch number: 025, Training: Loss: 0.8231, Accuracy: 0.6875\n",
      "Batch number: 026, Training: Loss: 1.0447, Accuracy: 0.6875\n",
      "Batch number: 027, Training: Loss: 0.7604, Accuracy: 0.7500\n",
      "Batch number: 028, Training: Loss: 1.0164, Accuracy: 0.5625\n",
      "Batch number: 029, Training: Loss: 2.1248, Accuracy: 0.4062\n",
      "Batch number: 030, Training: Loss: 1.3963, Accuracy: 0.6875\n",
      "Batch number: 031, Training: Loss: 0.8652, Accuracy: 0.7500\n",
      "Batch number: 032, Training: Loss: 0.7542, Accuracy: 0.8438\n",
      "Batch number: 033, Training: Loss: 1.7223, Accuracy: 0.5312\n",
      "Batch number: 034, Training: Loss: 2.0765, Accuracy: 0.4375\n",
      "Batch number: 035, Training: Loss: 0.9988, Accuracy: 0.6562\n",
      "Batch number: 036, Training: Loss: 1.2361, Accuracy: 0.5938\n",
      "Batch number: 037, Training: Loss: 0.6822, Accuracy: 0.7188\n",
      "Batch number: 038, Training: Loss: 0.9792, Accuracy: 0.6875\n",
      "Batch number: 039, Training: Loss: 0.8531, Accuracy: 0.8125\n",
      "Batch number: 040, Training: Loss: 1.2047, Accuracy: 0.6250\n",
      "Batch number: 041, Training: Loss: 0.7752, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 1.8410, Accuracy: 0.5312\n",
      "Batch number: 043, Training: Loss: 1.2590, Accuracy: 0.6562\n",
      "Batch number: 044, Training: Loss: 0.9558, Accuracy: 0.6875\n",
      "Batch number: 045, Training: Loss: 0.3754, Accuracy: 0.8750\n",
      "Batch number: 046, Training: Loss: 1.3895, Accuracy: 0.5000\n",
      "Batch number: 047, Training: Loss: 0.8914, Accuracy: 0.7188\n",
      "Batch number: 048, Training: Loss: 1.4491, Accuracy: 0.5625\n",
      "Batch number: 049, Training: Loss: 2.9970, Accuracy: 0.4688\n",
      "Batch number: 050, Training: Loss: 2.0067, Accuracy: 0.4688\n",
      "Batch number: 051, Training: Loss: 3.2025, Accuracy: 0.5000\n",
      "Batch number: 052, Training: Loss: 1.0773, Accuracy: 0.6562\n",
      "Batch number: 053, Training: Loss: 0.9159, Accuracy: 0.6875\n",
      "Batch number: 054, Training: Loss: 1.1550, Accuracy: 0.6250\n",
      "Batch number: 055, Training: Loss: 0.4993, Accuracy: 0.8438\n",
      "Batch number: 056, Training: Loss: 0.8832, Accuracy: 0.6875\n",
      "Batch number: 057, Training: Loss: 1.8058, Accuracy: 0.6250\n",
      "Batch number: 058, Training: Loss: 0.4253, Accuracy: 0.9062\n",
      "Batch number: 059, Training: Loss: 1.3221, Accuracy: 0.7188\n",
      "Batch number: 060, Training: Loss: 1.1696, Accuracy: 0.6875\n",
      "Batch number: 061, Training: Loss: 0.9010, Accuracy: 0.7188\n",
      "Batch number: 062, Training: Loss: 1.3144, Accuracy: 0.6250\n",
      "Batch number: 063, Training: Loss: 0.8446, Accuracy: 0.7188\n",
      "Batch number: 064, Training: Loss: 0.9055, Accuracy: 0.7188\n",
      "Batch number: 065, Training: Loss: 0.6110, Accuracy: 0.7812\n",
      "Batch number: 066, Training: Loss: 0.7097, Accuracy: 0.7188\n",
      "Batch number: 067, Training: Loss: 1.1468, Accuracy: 0.6562\n",
      "Batch number: 068, Training: Loss: 0.7618, Accuracy: 0.8125\n",
      "Batch number: 069, Training: Loss: 1.1533, Accuracy: 0.5938\n",
      "Batch number: 070, Training: Loss: 1.8126, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 1.9531, Accuracy: 0.5312\n",
      "Batch number: 072, Training: Loss: 1.0551, Accuracy: 0.6250\n",
      "Batch number: 073, Training: Loss: 2.0047, Accuracy: 0.5000\n",
      "Batch number: 074, Training: Loss: 1.0200, Accuracy: 0.6875\n",
      "Batch number: 075, Training: Loss: 1.3650, Accuracy: 0.6875\n",
      "Batch number: 076, Training: Loss: 0.6568, Accuracy: 0.7812\n",
      "Batch number: 077, Training: Loss: 1.5013, Accuracy: 0.5000\n",
      "Batch number: 078, Training: Loss: 1.5828, Accuracy: 0.6250\n",
      "Batch number: 079, Training: Loss: 0.6343, Accuracy: 0.8125\n",
      "Batch number: 080, Training: Loss: 0.9973, Accuracy: 0.7188\n",
      "Batch number: 081, Training: Loss: 0.2738, Accuracy: 0.8438\n",
      "Batch number: 082, Training: Loss: 0.4981, Accuracy: 0.8438\n",
      "Batch number: 083, Training: Loss: 0.3971, Accuracy: 0.8438\n",
      "Batch number: 084, Training: Loss: 0.6158, Accuracy: 0.7812\n",
      "Batch number: 085, Training: Loss: 0.6904, Accuracy: 0.7812\n",
      "Batch number: 086, Training: Loss: 0.5783, Accuracy: 0.7812\n",
      "Batch number: 087, Training: Loss: 0.9261, Accuracy: 0.7500\n",
      "Batch number: 088, Training: Loss: 0.3308, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 0.7756, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 1.1192, Accuracy: 0.6562\n",
      "Batch number: 091, Training: Loss: 0.5181, Accuracy: 0.7812\n",
      "Batch number: 092, Training: Loss: 1.0324, Accuracy: 0.6562\n",
      "Batch number: 093, Training: Loss: 0.6439, Accuracy: 0.8750\n",
      "Batch number: 094, Training: Loss: 1.2885, Accuracy: 0.6875\n",
      "Batch number: 095, Training: Loss: 0.9953, Accuracy: 0.6562\n",
      "Batch number: 096, Training: Loss: 0.8318, Accuracy: 0.7500\n",
      "Batch number: 097, Training: Loss: 0.9788, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 2.1754, Accuracy: 0.4062\n",
      "Batch number: 099, Training: Loss: 2.7787, Accuracy: 0.5312\n",
      "Batch number: 100, Training: Loss: 2.9546, Accuracy: 0.3438\n",
      "Batch number: 101, Training: Loss: 4.1668, Accuracy: 0.3125\n",
      "Batch number: 102, Training: Loss: 2.2140, Accuracy: 0.5625\n",
      "Batch number: 103, Training: Loss: 0.7025, Accuracy: 0.8438\n",
      "Batch number: 104, Training: Loss: 1.3459, Accuracy: 0.6562\n",
      "Batch number: 105, Training: Loss: 1.1421, Accuracy: 0.6562\n",
      "Batch number: 106, Training: Loss: 1.0590, Accuracy: 0.6250\n",
      "Batch number: 107, Training: Loss: 0.7042, Accuracy: 0.6875\n",
      "Batch number: 108, Training: Loss: 0.5101, Accuracy: 0.8438\n",
      "Batch number: 109, Training: Loss: 0.5796, Accuracy: 0.7812\n",
      "Batch number: 110, Training: Loss: 0.4494, Accuracy: 0.8125\n",
      "Batch number: 111, Training: Loss: 0.9175, Accuracy: 0.6250\n",
      "Batch number: 112, Training: Loss: 0.9974, Accuracy: 0.5625\n",
      "Batch number: 113, Training: Loss: 1.2948, Accuracy: 0.6875\n",
      "Batch number: 114, Training: Loss: 1.0542, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 1.1109, Accuracy: 0.6875\n",
      "Batch number: 116, Training: Loss: 0.4374, Accuracy: 0.8125\n",
      "Batch number: 117, Training: Loss: 1.3359, Accuracy: 0.7188\n",
      "Batch number: 118, Training: Loss: 0.8321, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.5699, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.6753, Accuracy: 0.8438\n",
      "Batch number: 121, Training: Loss: 0.9041, Accuracy: 0.8125\n",
      "Batch number: 122, Training: Loss: 0.9888, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 0.7636, Accuracy: 0.7188\n",
      "Batch number: 124, Training: Loss: 0.7464, Accuracy: 0.7500\n",
      "Batch number: 125, Training: Loss: 0.2119, Accuracy: 0.9062\n",
      "Batch number: 126, Training: Loss: 0.8702, Accuracy: 0.6875\n",
      "Batch number: 127, Training: Loss: 0.4833, Accuracy: 0.8125\n",
      "Batch number: 128, Training: Loss: 1.1811, Accuracy: 0.6250\n",
      "Batch number: 129, Training: Loss: 0.9964, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 1.0882, Accuracy: 0.6875\n",
      "Batch number: 131, Training: Loss: 0.4489, Accuracy: 0.8333\n",
      "Epoch: 4/5\n",
      "Batch number: 000, Training: Loss: 1.3286, Accuracy: 0.6562\n",
      "Batch number: 001, Training: Loss: 0.6901, Accuracy: 0.7812\n",
      "Batch number: 002, Training: Loss: 0.8117, Accuracy: 0.7812\n",
      "Batch number: 003, Training: Loss: 1.3334, Accuracy: 0.7188\n",
      "Batch number: 004, Training: Loss: 0.4633, Accuracy: 0.8125\n",
      "Batch number: 005, Training: Loss: 0.4746, Accuracy: 0.8125\n",
      "Batch number: 006, Training: Loss: 0.9599, Accuracy: 0.7188\n",
      "Batch number: 007, Training: Loss: 0.7309, Accuracy: 0.7188\n",
      "Batch number: 008, Training: Loss: 0.8516, Accuracy: 0.7188\n",
      "Batch number: 009, Training: Loss: 0.7809, Accuracy: 0.7500\n",
      "Batch number: 010, Training: Loss: 0.8280, Accuracy: 0.7188\n",
      "Batch number: 011, Training: Loss: 2.0780, Accuracy: 0.4375\n",
      "Batch number: 012, Training: Loss: 2.0134, Accuracy: 0.7188\n",
      "Batch number: 013, Training: Loss: 1.1083, Accuracy: 0.6875\n",
      "Batch number: 014, Training: Loss: 0.4047, Accuracy: 0.8438\n",
      "Batch number: 015, Training: Loss: 2.2077, Accuracy: 0.5312\n",
      "Batch number: 016, Training: Loss: 1.9867, Accuracy: 0.5312\n",
      "Batch number: 017, Training: Loss: 0.9916, Accuracy: 0.6875\n",
      "Batch number: 018, Training: Loss: 0.9535, Accuracy: 0.6250\n",
      "Batch number: 019, Training: Loss: 1.3287, Accuracy: 0.5312\n",
      "Batch number: 020, Training: Loss: 1.0402, Accuracy: 0.7812\n",
      "Batch number: 021, Training: Loss: 2.1427, Accuracy: 0.5000\n",
      "Batch number: 022, Training: Loss: 1.3591, Accuracy: 0.7500\n",
      "Batch number: 023, Training: Loss: 1.2086, Accuracy: 0.7812\n",
      "Batch number: 024, Training: Loss: 0.6264, Accuracy: 0.8125\n",
      "Batch number: 025, Training: Loss: 0.7076, Accuracy: 0.7188\n",
      "Batch number: 026, Training: Loss: 0.6524, Accuracy: 0.8125\n",
      "Batch number: 027, Training: Loss: 1.1288, Accuracy: 0.6875\n",
      "Batch number: 028, Training: Loss: 0.9248, Accuracy: 0.6562\n",
      "Batch number: 029, Training: Loss: 0.8198, Accuracy: 0.8125\n",
      "Batch number: 030, Training: Loss: 0.8476, Accuracy: 0.7812\n",
      "Batch number: 031, Training: Loss: 1.2568, Accuracy: 0.6562\n",
      "Batch number: 032, Training: Loss: 0.9208, Accuracy: 0.6875\n",
      "Batch number: 033, Training: Loss: 1.6120, Accuracy: 0.5312\n",
      "Batch number: 034, Training: Loss: 3.4171, Accuracy: 0.3438\n",
      "Batch number: 035, Training: Loss: 2.2313, Accuracy: 0.4062\n",
      "Batch number: 036, Training: Loss: 1.0844, Accuracy: 0.5938\n",
      "Batch number: 037, Training: Loss: 1.8975, Accuracy: 0.6250\n",
      "Batch number: 038, Training: Loss: 0.6530, Accuracy: 0.6875\n",
      "Batch number: 039, Training: Loss: 0.7087, Accuracy: 0.8438\n",
      "Batch number: 040, Training: Loss: 2.0444, Accuracy: 0.5312\n",
      "Batch number: 041, Training: Loss: 1.4496, Accuracy: 0.5938\n",
      "Batch number: 042, Training: Loss: 2.3591, Accuracy: 0.4688\n",
      "Batch number: 043, Training: Loss: 2.4602, Accuracy: 0.5000\n",
      "Batch number: 044, Training: Loss: 0.7403, Accuracy: 0.7812\n",
      "Batch number: 045, Training: Loss: 2.6827, Accuracy: 0.4375\n",
      "Batch number: 046, Training: Loss: 0.5331, Accuracy: 0.7188\n",
      "Batch number: 047, Training: Loss: 0.6468, Accuracy: 0.6875\n",
      "Batch number: 048, Training: Loss: 0.6281, Accuracy: 0.7812\n",
      "Batch number: 049, Training: Loss: 0.9508, Accuracy: 0.7500\n",
      "Batch number: 050, Training: Loss: 0.6459, Accuracy: 0.7812\n",
      "Batch number: 051, Training: Loss: 1.0082, Accuracy: 0.5938\n",
      "Batch number: 052, Training: Loss: 0.4201, Accuracy: 0.8438\n",
      "Batch number: 053, Training: Loss: 1.4292, Accuracy: 0.5000\n",
      "Batch number: 054, Training: Loss: 0.4617, Accuracy: 0.8125\n",
      "Batch number: 055, Training: Loss: 1.6393, Accuracy: 0.5625\n",
      "Batch number: 056, Training: Loss: 0.5113, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 1.5928, Accuracy: 0.5938\n",
      "Batch number: 058, Training: Loss: 0.6281, Accuracy: 0.8438\n",
      "Batch number: 059, Training: Loss: 0.7251, Accuracy: 0.7188\n",
      "Batch number: 060, Training: Loss: 0.9056, Accuracy: 0.7188\n",
      "Batch number: 061, Training: Loss: 0.6629, Accuracy: 0.8125\n",
      "Batch number: 062, Training: Loss: 1.4756, Accuracy: 0.5625\n",
      "Batch number: 063, Training: Loss: 0.4103, Accuracy: 0.9062\n",
      "Batch number: 064, Training: Loss: 1.3533, Accuracy: 0.5312\n",
      "Batch number: 065, Training: Loss: 1.1687, Accuracy: 0.7500\n",
      "Batch number: 066, Training: Loss: 0.8815, Accuracy: 0.7812\n",
      "Batch number: 067, Training: Loss: 0.7940, Accuracy: 0.7812\n",
      "Batch number: 068, Training: Loss: 1.0775, Accuracy: 0.7188\n",
      "Batch number: 069, Training: Loss: 0.4107, Accuracy: 0.8438\n",
      "Batch number: 070, Training: Loss: 0.9293, Accuracy: 0.6562\n",
      "Batch number: 071, Training: Loss: 1.0500, Accuracy: 0.6562\n",
      "Batch number: 072, Training: Loss: 1.8415, Accuracy: 0.5000\n",
      "Batch number: 073, Training: Loss: 1.0142, Accuracy: 0.7188\n",
      "Batch number: 074, Training: Loss: 0.8128, Accuracy: 0.7188\n",
      "Batch number: 075, Training: Loss: 1.2299, Accuracy: 0.5625\n",
      "Batch number: 076, Training: Loss: 0.7285, Accuracy: 0.7188\n",
      "Batch number: 077, Training: Loss: 1.1691, Accuracy: 0.5938\n",
      "Batch number: 078, Training: Loss: 1.3545, Accuracy: 0.5938\n",
      "Batch number: 079, Training: Loss: 1.5311, Accuracy: 0.6562\n",
      "Batch number: 080, Training: Loss: 3.8496, Accuracy: 0.3438\n",
      "Batch number: 081, Training: Loss: 0.6876, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 2.5296, Accuracy: 0.4375\n",
      "Batch number: 083, Training: Loss: 1.4740, Accuracy: 0.5312\n",
      "Batch number: 084, Training: Loss: 0.8787, Accuracy: 0.6875\n",
      "Batch number: 085, Training: Loss: 0.6874, Accuracy: 0.7812\n",
      "Batch number: 086, Training: Loss: 0.9786, Accuracy: 0.6875\n",
      "Batch number: 087, Training: Loss: 1.6026, Accuracy: 0.5938\n",
      "Batch number: 088, Training: Loss: 1.2667, Accuracy: 0.6250\n",
      "Batch number: 089, Training: Loss: 0.7993, Accuracy: 0.7500\n",
      "Batch number: 090, Training: Loss: 0.4070, Accuracy: 0.8750\n",
      "Batch number: 091, Training: Loss: 0.6379, Accuracy: 0.7188\n",
      "Batch number: 092, Training: Loss: 0.9176, Accuracy: 0.6875\n",
      "Batch number: 093, Training: Loss: 1.1845, Accuracy: 0.6250\n",
      "Batch number: 094, Training: Loss: 1.0684, Accuracy: 0.6875\n",
      "Batch number: 095, Training: Loss: 0.5829, Accuracy: 0.7188\n",
      "Batch number: 096, Training: Loss: 0.3636, Accuracy: 0.9062\n",
      "Batch number: 097, Training: Loss: 0.8496, Accuracy: 0.7500\n",
      "Batch number: 098, Training: Loss: 1.2152, Accuracy: 0.6250\n",
      "Batch number: 099, Training: Loss: 0.6105, Accuracy: 0.7500\n",
      "Batch number: 100, Training: Loss: 0.6049, Accuracy: 0.7812\n",
      "Batch number: 101, Training: Loss: 1.0677, Accuracy: 0.6562\n",
      "Batch number: 102, Training: Loss: 0.3844, Accuracy: 0.8438\n",
      "Batch number: 103, Training: Loss: 1.5847, Accuracy: 0.5625\n",
      "Batch number: 104, Training: Loss: 1.0916, Accuracy: 0.5000\n",
      "Batch number: 105, Training: Loss: 0.8178, Accuracy: 0.7500\n",
      "Batch number: 106, Training: Loss: 0.5731, Accuracy: 0.8125\n",
      "Batch number: 107, Training: Loss: 0.5767, Accuracy: 0.7812\n",
      "Batch number: 108, Training: Loss: 1.1256, Accuracy: 0.6562\n",
      "Batch number: 109, Training: Loss: 1.1776, Accuracy: 0.6875\n",
      "Batch number: 110, Training: Loss: 0.9121, Accuracy: 0.8125\n",
      "Batch number: 111, Training: Loss: 0.6033, Accuracy: 0.7812\n",
      "Batch number: 112, Training: Loss: 2.3643, Accuracy: 0.4375\n",
      "Batch number: 113, Training: Loss: 3.6575, Accuracy: 0.4375\n",
      "Batch number: 114, Training: Loss: 0.6278, Accuracy: 0.7812\n",
      "Batch number: 115, Training: Loss: 1.2672, Accuracy: 0.6562\n",
      "Batch number: 116, Training: Loss: 0.9754, Accuracy: 0.7500\n",
      "Batch number: 117, Training: Loss: 1.1346, Accuracy: 0.6562\n",
      "Batch number: 118, Training: Loss: 0.4442, Accuracy: 0.8438\n",
      "Batch number: 119, Training: Loss: 2.0550, Accuracy: 0.5312\n",
      "Batch number: 120, Training: Loss: 1.2559, Accuracy: 0.6250\n",
      "Batch number: 121, Training: Loss: 1.2100, Accuracy: 0.6250\n",
      "Batch number: 122, Training: Loss: 0.9965, Accuracy: 0.7500\n",
      "Batch number: 123, Training: Loss: 0.3788, Accuracy: 0.8750\n",
      "Batch number: 124, Training: Loss: 0.6762, Accuracy: 0.7500\n",
      "Batch number: 125, Training: Loss: 1.0648, Accuracy: 0.6250\n",
      "Batch number: 126, Training: Loss: 1.6238, Accuracy: 0.5625\n",
      "Batch number: 127, Training: Loss: 1.2899, Accuracy: 0.6250\n",
      "Batch number: 128, Training: Loss: 1.9345, Accuracy: 0.5000\n",
      "Batch number: 129, Training: Loss: 1.0272, Accuracy: 0.6875\n",
      "Batch number: 130, Training: Loss: 1.4665, Accuracy: 0.6875\n",
      "Batch number: 131, Training: Loss: 0.1568, Accuracy: 1.0000\n",
      "Epoch: 5/5\n",
      "Batch number: 000, Training: Loss: 1.9874, Accuracy: 0.6250\n",
      "Batch number: 001, Training: Loss: 0.7630, Accuracy: 0.7812\n",
      "Batch number: 002, Training: Loss: 0.6456, Accuracy: 0.8438\n",
      "Batch number: 003, Training: Loss: 1.3146, Accuracy: 0.6250\n",
      "Batch number: 004, Training: Loss: 1.3497, Accuracy: 0.6250\n",
      "Batch number: 005, Training: Loss: 0.6890, Accuracy: 0.7500\n",
      "Batch number: 006, Training: Loss: 0.4992, Accuracy: 0.8438\n",
      "Batch number: 007, Training: Loss: 0.3909, Accuracy: 0.9062\n",
      "Batch number: 008, Training: Loss: 0.2018, Accuracy: 0.9375\n",
      "Batch number: 009, Training: Loss: 0.6005, Accuracy: 0.8125\n",
      "Batch number: 010, Training: Loss: 0.3519, Accuracy: 0.8750\n",
      "Batch number: 011, Training: Loss: 0.7700, Accuracy: 0.7188\n",
      "Batch number: 012, Training: Loss: 0.2049, Accuracy: 0.8750\n",
      "Batch number: 013, Training: Loss: 0.5562, Accuracy: 0.8438\n",
      "Batch number: 014, Training: Loss: 1.0215, Accuracy: 0.6875\n",
      "Batch number: 015, Training: Loss: 0.8174, Accuracy: 0.7812\n",
      "Batch number: 016, Training: Loss: 0.9192, Accuracy: 0.7500\n",
      "Batch number: 017, Training: Loss: 0.3834, Accuracy: 0.8750\n",
      "Batch number: 018, Training: Loss: 1.0303, Accuracy: 0.7188\n",
      "Batch number: 019, Training: Loss: 0.8558, Accuracy: 0.7188\n",
      "Batch number: 020, Training: Loss: 0.7005, Accuracy: 0.8438\n",
      "Batch number: 021, Training: Loss: 0.6239, Accuracy: 0.7812\n",
      "Batch number: 022, Training: Loss: 0.6263, Accuracy: 0.6875\n",
      "Batch number: 023, Training: Loss: 0.3987, Accuracy: 0.8438\n",
      "Batch number: 024, Training: Loss: 0.9351, Accuracy: 0.6562\n",
      "Batch number: 025, Training: Loss: 0.5560, Accuracy: 0.8750\n",
      "Batch number: 026, Training: Loss: 0.8206, Accuracy: 0.6562\n",
      "Batch number: 027, Training: Loss: 1.8497, Accuracy: 0.5312\n",
      "Batch number: 028, Training: Loss: 1.2236, Accuracy: 0.6250\n",
      "Batch number: 029, Training: Loss: 1.2425, Accuracy: 0.6250\n",
      "Batch number: 030, Training: Loss: 1.7567, Accuracy: 0.5938\n",
      "Batch number: 031, Training: Loss: 2.6222, Accuracy: 0.5000\n",
      "Batch number: 032, Training: Loss: 0.7883, Accuracy: 0.7500\n",
      "Batch number: 033, Training: Loss: 0.6671, Accuracy: 0.7188\n",
      "Batch number: 034, Training: Loss: 0.5739, Accuracy: 0.8125\n",
      "Batch number: 035, Training: Loss: 0.5139, Accuracy: 0.8125\n",
      "Batch number: 036, Training: Loss: 0.7222, Accuracy: 0.7188\n",
      "Batch number: 037, Training: Loss: 0.6878, Accuracy: 0.7812\n",
      "Batch number: 038, Training: Loss: 1.0414, Accuracy: 0.6562\n",
      "Batch number: 039, Training: Loss: 1.0738, Accuracy: 0.6875\n",
      "Batch number: 040, Training: Loss: 0.4085, Accuracy: 0.8438\n",
      "Batch number: 041, Training: Loss: 0.7818, Accuracy: 0.7500\n",
      "Batch number: 042, Training: Loss: 0.5784, Accuracy: 0.8438\n",
      "Batch number: 043, Training: Loss: 0.5813, Accuracy: 0.7188\n",
      "Batch number: 044, Training: Loss: 0.8461, Accuracy: 0.7500\n",
      "Batch number: 045, Training: Loss: 0.5998, Accuracy: 0.8438\n",
      "Batch number: 046, Training: Loss: 0.4096, Accuracy: 0.8750\n",
      "Batch number: 047, Training: Loss: 0.9198, Accuracy: 0.5938\n",
      "Batch number: 048, Training: Loss: 0.6700, Accuracy: 0.8125\n",
      "Batch number: 049, Training: Loss: 0.6757, Accuracy: 0.8125\n",
      "Batch number: 050, Training: Loss: 1.0944, Accuracy: 0.5938\n",
      "Batch number: 051, Training: Loss: 0.9005, Accuracy: 0.7188\n",
      "Batch number: 052, Training: Loss: 1.4070, Accuracy: 0.6250\n",
      "Batch number: 053, Training: Loss: 0.5766, Accuracy: 0.8438\n",
      "Batch number: 054, Training: Loss: 1.3019, Accuracy: 0.6562\n",
      "Batch number: 055, Training: Loss: 0.8527, Accuracy: 0.6250\n",
      "Batch number: 056, Training: Loss: 0.8230, Accuracy: 0.7500\n",
      "Batch number: 057, Training: Loss: 0.6445, Accuracy: 0.8125\n",
      "Batch number: 058, Training: Loss: 1.0046, Accuracy: 0.6250\n",
      "Batch number: 059, Training: Loss: 0.8781, Accuracy: 0.8438\n",
      "Batch number: 060, Training: Loss: 0.9673, Accuracy: 0.5938\n",
      "Batch number: 061, Training: Loss: 0.6685, Accuracy: 0.8438\n",
      "Batch number: 062, Training: Loss: 0.5987, Accuracy: 0.7812\n",
      "Batch number: 063, Training: Loss: 0.6642, Accuracy: 0.7500\n",
      "Batch number: 064, Training: Loss: 0.7737, Accuracy: 0.8125\n",
      "Batch number: 065, Training: Loss: 0.5940, Accuracy: 0.7812\n",
      "Batch number: 066, Training: Loss: 0.7344, Accuracy: 0.7500\n",
      "Batch number: 067, Training: Loss: 0.7196, Accuracy: 0.7188\n",
      "Batch number: 068, Training: Loss: 1.2894, Accuracy: 0.6875\n",
      "Batch number: 069, Training: Loss: 0.6955, Accuracy: 0.7812\n",
      "Batch number: 070, Training: Loss: 1.4394, Accuracy: 0.5938\n",
      "Batch number: 071, Training: Loss: 1.0563, Accuracy: 0.7500\n",
      "Batch number: 072, Training: Loss: 0.4580, Accuracy: 0.8438\n",
      "Batch number: 073, Training: Loss: 0.8753, Accuracy: 0.6875\n",
      "Batch number: 074, Training: Loss: 2.6441, Accuracy: 0.4688\n",
      "Batch number: 075, Training: Loss: 1.3766, Accuracy: 0.7500\n",
      "Batch number: 076, Training: Loss: 2.3582, Accuracy: 0.5625\n",
      "Batch number: 077, Training: Loss: 0.8788, Accuracy: 0.5312\n",
      "Batch number: 078, Training: Loss: 0.7794, Accuracy: 0.7188\n",
      "Batch number: 079, Training: Loss: 0.8024, Accuracy: 0.6875\n",
      "Batch number: 080, Training: Loss: 0.1466, Accuracy: 0.9688\n",
      "Batch number: 081, Training: Loss: 1.0449, Accuracy: 0.7500\n",
      "Batch number: 082, Training: Loss: 1.1649, Accuracy: 0.7500\n",
      "Batch number: 083, Training: Loss: 0.4049, Accuracy: 0.9062\n",
      "Batch number: 084, Training: Loss: 0.2572, Accuracy: 0.9062\n",
      "Batch number: 085, Training: Loss: 0.9538, Accuracy: 0.6250\n",
      "Batch number: 086, Training: Loss: 0.4990, Accuracy: 0.7500\n",
      "Batch number: 087, Training: Loss: 0.9082, Accuracy: 0.6562\n",
      "Batch number: 088, Training: Loss: 0.5038, Accuracy: 0.8750\n",
      "Batch number: 089, Training: Loss: 1.9676, Accuracy: 0.5000\n",
      "Batch number: 090, Training: Loss: 1.3748, Accuracy: 0.5938\n",
      "Batch number: 091, Training: Loss: 1.0620, Accuracy: 0.6875\n",
      "Batch number: 092, Training: Loss: 0.4508, Accuracy: 0.8125\n",
      "Batch number: 093, Training: Loss: 1.6413, Accuracy: 0.5312\n",
      "Batch number: 094, Training: Loss: 0.5689, Accuracy: 0.8125\n",
      "Batch number: 095, Training: Loss: 0.9356, Accuracy: 0.8125\n",
      "Batch number: 096, Training: Loss: 1.2945, Accuracy: 0.6250\n",
      "Batch number: 097, Training: Loss: 2.0248, Accuracy: 0.5000\n",
      "Batch number: 098, Training: Loss: 1.4081, Accuracy: 0.6250\n",
      "Batch number: 099, Training: Loss: 0.5604, Accuracy: 0.7812\n",
      "Batch number: 100, Training: Loss: 1.0424, Accuracy: 0.7188\n",
      "Batch number: 101, Training: Loss: 0.3803, Accuracy: 0.8750\n",
      "Batch number: 102, Training: Loss: 1.7486, Accuracy: 0.5000\n",
      "Batch number: 103, Training: Loss: 1.3962, Accuracy: 0.6562\n",
      "Batch number: 104, Training: Loss: 1.8204, Accuracy: 0.6250\n",
      "Batch number: 105, Training: Loss: 0.3199, Accuracy: 0.9375\n",
      "Batch number: 106, Training: Loss: 1.3345, Accuracy: 0.6875\n",
      "Batch number: 107, Training: Loss: 0.9584, Accuracy: 0.6875\n",
      "Batch number: 108, Training: Loss: 0.5193, Accuracy: 0.8438\n",
      "Batch number: 109, Training: Loss: 0.8437, Accuracy: 0.6562\n",
      "Batch number: 110, Training: Loss: 0.9173, Accuracy: 0.7188\n",
      "Batch number: 111, Training: Loss: 0.8267, Accuracy: 0.8125\n",
      "Batch number: 112, Training: Loss: 0.7878, Accuracy: 0.7188\n",
      "Batch number: 113, Training: Loss: 0.5102, Accuracy: 0.8125\n",
      "Batch number: 114, Training: Loss: 0.6508, Accuracy: 0.7500\n",
      "Batch number: 115, Training: Loss: 1.0643, Accuracy: 0.7188\n",
      "Batch number: 116, Training: Loss: 0.7410, Accuracy: 0.6562\n",
      "Batch number: 117, Training: Loss: 0.5562, Accuracy: 0.7812\n",
      "Batch number: 118, Training: Loss: 0.8503, Accuracy: 0.7188\n",
      "Batch number: 119, Training: Loss: 0.3937, Accuracy: 0.8125\n",
      "Batch number: 120, Training: Loss: 0.5671, Accuracy: 0.8125\n",
      "Batch number: 121, Training: Loss: 0.6611, Accuracy: 0.7188\n",
      "Batch number: 122, Training: Loss: 1.0214, Accuracy: 0.7188\n",
      "Batch number: 123, Training: Loss: 0.8713, Accuracy: 0.7188\n",
      "Batch number: 124, Training: Loss: 1.0362, Accuracy: 0.6875\n",
      "Batch number: 125, Training: Loss: 0.5313, Accuracy: 0.8125\n",
      "Batch number: 126, Training: Loss: 1.1801, Accuracy: 0.7500\n",
      "Batch number: 127, Training: Loss: 0.8965, Accuracy: 0.7188\n",
      "Batch number: 128, Training: Loss: 0.7110, Accuracy: 0.7188\n",
      "Batch number: 129, Training: Loss: 0.3296, Accuracy: 0.8125\n",
      "Batch number: 130, Training: Loss: 0.7488, Accuracy: 0.6875\n",
      "Batch number: 131, Training: Loss: 1.4270, Accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "for epoch in range(n_epochs):\n",
    "        epoch_start = time.time()\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, n_epochs))\n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        valid_loss = 0.0\n",
    "        valid_acc = 0.0\n",
    "        for i, (inputs, labels) in enumerate(data_loaders[\"train\"]):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Clean existing gradients\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass - compute outputs on input data using the model\n",
    "            outputs = model(inputs)\n",
    "            # Compute loss\n",
    "            losst = loss(outputs, labels)\n",
    "            # Backpropagate the gradients\n",
    "            losst.backward()\n",
    "            # Update the parameters\n",
    "            optimizer.step()\n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += losst.item() * inputs.size(0)\n",
    "            # Compute the accuracy\n",
    "            ret, predictions = torch.max(outputs.data, 1)\n",
    "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * inputs.size(0)\n",
    "            print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, losst.item(), acc.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_model = './model.pth'\n",
    "\n",
    "# save\n",
    "def save(model, optimizer):\n",
    "    # save\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, output_model)\n",
    "\n",
    "save(model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "qXb0DH7hz4qU",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Froze 161 groups of parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST\n",
    "#****************\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "n_inputs = model.fc.in_features\n",
    "\n",
    "# Feel free to experiment with more complicated heads\n",
    "model.fc = nn.Linear(n_inputs, n_classes)\n",
    "\n",
    "frozen_parameters = []\n",
    "for p in model.parameters():\n",
    "    # Freeze only parameters that are not already frozen\n",
    "    # (if any)\n",
    "    if p.requires_grad:\n",
    "        p.requires_grad = False\n",
    "        frozen_parameters.append(p)\n",
    "\n",
    "print(f\"Froze {len(frozen_parameters)} groups of parameters\")\n",
    "\n",
    "# Now let's thaw the parameters of the head we have\n",
    "# added\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "# YOUR CODE HERE: load the weights in 'checkpoints/best_val_loss.pt'\n",
    "model.load_state_dict(torch.load(output_model)[\"model_state_dict\"])\n",
    "\n",
    "# Run test\n",
    "#_ = one_epoch_test(data_loaders['test'], model, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYwhmPbhlEIw",
    "outputId": "ee25d967-9c64-4771-e71d-a5fb02d5c46d",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# EXPORT USING TORCHSCRIPT\n",
    "#*********************************\n",
    "class Predictor(nn.Module):\n",
    "\n",
    "    def __init__(self, model, class_names, mean, std):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model.eval()\n",
    "        self.class_names = class_names\n",
    "\n",
    "        # We use nn.Sequential and not nn.Compose because the former\n",
    "        # is compatible with torch.script, while the latter isn't\n",
    "        self.transforms = nn.Sequential(\n",
    "            T.Resize([256, ]),  # We use single int value inside a list due to torchscript type restrictions\n",
    "            T.CenterCrop(224),\n",
    "            T.ConvertImageDtype(torch.float),\n",
    "            T.Normalize(mean.tolist(), std.tolist())\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        with torch.no_grad():\n",
    "            # 1. apply transforms\n",
    "            x  = self.transforms(x)\n",
    "            # 2. get the logits\n",
    "            x  = self.model(x) \n",
    "            # 3. apply softmax\n",
    "            x  = F.softmax(x, dim=1)\n",
    "\n",
    "            return x\n",
    "\n",
    "# First let's get the class names from our data loaders\n",
    "class_names = data_loaders[\"train\"].dataset.classes\n",
    "\n",
    "# Then let's move the model to the CPU\n",
    "# (we don't need GPU for inference)\n",
    "model = model.cpu()\n",
    "# Let's make sure we use the right weights by loading the\n",
    "# best weights we have found during training\n",
    "# NOTE: remember to use map_location='cpu' so the weights\n",
    "# are loaded on the CPU (and not the GPU)\n",
    "model.load_state_dict(\n",
    "    torch.load(\"./model.pth\", map_location=\"cpu\")[\"model_state_dict\"]\n",
    ")\n",
    "\n",
    "# Let's wrap our model using the predictor class\n",
    "#mean, std = compute_mean_and_std()\n",
    "#predictor = Predictor(model, class_names, mean, std).cpu()\n",
    "\n",
    "# Export using torch.jit.script\n",
    "#scripted_predictor = torch.jit.script(predictor)\n",
    "#scripted_predictor.save(\"./model_exported.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "pCamoDf1obOu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST EXPORTED MODEL\n",
    "#*************************\n",
    "#from tqdm import tqdm\n",
    "\n",
    "#def predictor_test(test_dataloader, model_reloaded):\n",
    "#    \"\"\"\n",
    "#    Test the predictor. Since the predictor does not operate on the same tensors\n",
    "#    as the non-wrapped model, we need a specific test function (can't use one_epoch_test)\n",
    "#    \"\"\"\n",
    "\n",
    "    #folder = get_data_location()\n",
    "    #test_data = datasets.ImageFolder(os.path.join(folder, \"test\"), transform=T.ToTensor())\n",
    "#    test_data = datasets.ImageFolder(root=test_directory, transform=T.ToTensor())\n",
    "\n",
    "#    pred = []\n",
    "#    truth = []\n",
    "#    for x in tqdm(test_data, total=len(test_dataloader.dataset), leave=True, ncols=80):\n",
    "#        softmax = model_reloaded(x[0].unsqueeze(dim=0))\n",
    "\n",
    "#        idx = softmax.squeeze().argmax()\n",
    "\n",
    "#        pred.append(int(x[1]))\n",
    "#        truth.append(int(idx))\n",
    "\n",
    "#    pred = np.array(pred)\n",
    "#    truth = np.array(truth)\n",
    "\n",
    "#    print(f\"Accuracy: {(pred==truth).sum() / pred.shape[0]}\")\n",
    "\n",
    "#    return truth, pred\n",
    "\n",
    "#model_reloaded = torch.load(\"./model.pth\")\n",
    "\n",
    "#pred, truth = predictor_test(data_loaders['test'], model_reloaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "id": "BUQXzmqT7qRV",
    "outputId": "09ab2348-2c61-4744-e78a-1686809175b7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#classes = data_loaders[\"train\"].dataset.classes\n",
    "\n",
    "#plot_confusion_matrix(pred, truth, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "# iterate over test data\n",
    "for inputs, labels in test:\n",
    "        output = model(inputs) # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output) # Save Prediction\n",
    "        \n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels) # Save Truth\n",
    "\n",
    "# constant for classes\n",
    "classes = data_loaders[\"train\"].dataset.classes\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "#plt.savefig('output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.8 Python 3.6 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/1.8.1-cpu-py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a9fbc0036d54628adabdb11333ac239": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5fd5a2129451462fbaf6102ebcbdf5e4",
      "max": 102530333,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_de62d745176a45cbb717a4ac0e01c663",
      "value": 102530333
     }
    },
    "21d94565226c4382a1246fc35ee07af3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6b38c4778ce84a898c3d8a48c551868e",
      "placeholder": "​",
      "style": "IPY_MODEL_48fbf4f41d504a1c80c2ebef4f7cf89c",
      "value": "100%"
     }
    },
    "48fbf4f41d504a1c80c2ebef4f7cf89c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fd5a2129451462fbaf6102ebcbdf5e4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65c94d6ae1ad4259b4a2bf6eae9a05a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6b38c4778ce84a898c3d8a48c551868e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "832c967da8344db58ef0c6ee528ec936": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_65c94d6ae1ad4259b4a2bf6eae9a05a3",
      "placeholder": "​",
      "style": "IPY_MODEL_bf5dee7f55aa41089f82f50415a2d34d",
      "value": " 97.8M/97.8M [00:00&lt;00:00, 213MB/s]"
     }
    },
    "8935e48ec10441be950c56e86b2ddda5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21d94565226c4382a1246fc35ee07af3",
       "IPY_MODEL_1a9fbc0036d54628adabdb11333ac239",
       "IPY_MODEL_832c967da8344db58ef0c6ee528ec936"
      ],
      "layout": "IPY_MODEL_bc723b18a6eb4332a77b371e77244b34"
     }
    },
    "bc723b18a6eb4332a77b371e77244b34": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bf5dee7f55aa41089f82f50415a2d34d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "de62d745176a45cbb717a4ac0e01c663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
